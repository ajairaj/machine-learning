{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import net_utils as nu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load cifar-10 dataset\n",
    "(x_train, y_train),(x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "onehot_train = nu.convert_to_onehot(y_train, 10)\n",
    "onehot_test = nu.convert_to_onehot(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### visualize some of the images\n",
    "fig, axes = plt.subplots(2,10,figsize=(20,5))\n",
    "ctr = 0;\n",
    "for i in range(2):\n",
    "    for j in range(10):\n",
    "        axes[i,j].imshow(np.squeeze(x_train[ctr]))\n",
    "        axes[i,j].axes.get_xaxis().set_visible(False)\n",
    "        axes[i,j].axes.get_yaxis().set_visible(False)\n",
    "        axes[i,j].set_title(str(y_train[ctr]))\n",
    "        ctr += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from basic_cnn import basic_cnn as basic_cnn\n",
    "import tqdm\n",
    "import time\n",
    "\n",
    "class conv_net:\n",
    "    def __init__(self, im_size, n_chan, n_class, learning_rate):\n",
    "        # settings\n",
    "        self.im_size = im_size\n",
    "        self.n_chan = n_chan\n",
    "        self.n_class = n_class\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # placeholders for inputs and ouputs\n",
    "        self.inputs = tf.placeholder(tf.float32, (None, im_size, im_size, n_chan), name=\"inputs\")\n",
    "        self.targets = tf.placeholder(tf.float32, (None, n_class), name=\"targets\")\n",
    "        pred, cost = basic_cnn(self.inputs, self.targets)\n",
    "        self.pred = pred\n",
    "        self.cost = cost\n",
    "        \n",
    "        # optimization function\n",
    "        vars_to_minimize = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='conv_net')\n",
    "        self.opt = tf.train.AdamOptimizer(self.learning_rate).minimize(cost, var_list=vars_to_minimize)\n",
    "        \n",
    "        # metrics\n",
    "        self.auc_list = []\n",
    "        self.auc_op_list = []\n",
    "        self.f1s_list = []\n",
    "        self.f1s_op_list = []\n",
    "        \n",
    "        for i in range(self.n_class):\n",
    "            auc, auc_op = tf.metrics.auc(self.targets[:,i], self.pred[:,i])\n",
    "            self.auc_list.append(auc)\n",
    "            self.auc_op_list.append(auc_op)\n",
    "            f1s, f1s_op =  tf.contrib.metrics.f1_score(self.targets[:,i], self.pred[:,i])\n",
    "            self.f1s_list.append(f1s)\n",
    "            self.f1s_op_list.append(f1s_op)\n",
    "            \n",
    "        self.avg_auc = tf.add_n(self.auc_list) / n_class\n",
    "        self.avg_f1s = tf.add_n(self.f1s_list) / n_class\n",
    "        \n",
    "    def parameter_count(self, full=False):\n",
    "        vars_to_minimize = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='conv_net')\n",
    "        nu.parameter_count(vars_to_minimize, full = False)\n",
    "        \n",
    "    def train_on_batch(self, inputs, targets, mb_sz = 32):\n",
    "        data_count = inputs.shape[0]\n",
    "\n",
    "        for i in range(0, data_count, mb_sz):\n",
    "            i_s = i\n",
    "            i_e = np.minimum(data_count, i + mb_sz)\n",
    "            sess_out = self.opt\n",
    "            feed_dict={self.inputs: inputs[i_s:i_e], self.targets: targets[i_s:i_e]}\n",
    "            sess.run(sess_out, feed_dict=feed_dict)\n",
    "            time.sleep(0.01)\n",
    "            \n",
    "    def run_on_batch(self, inputs, targets, mb_sz = 32):\n",
    "        data_count = inputs.shape[0]\n",
    "        pred = np.zeros((data_count, self.n_class))\n",
    "        cost = 0\n",
    "\n",
    "        for i in range(0, data_count, mb_sz):\n",
    "            i_s = i\n",
    "            i_e = np.minimum(data_count, i + mb_sz)\n",
    "            \n",
    "            sess_out = [self.pred, self.cost]\n",
    "            feed_dict={self.inputs: inputs[i_s:i_e], self.targets: targets[i_s:i_e]}\n",
    "            results = sess.run(sess_out, feed_dict=feed_dict)\n",
    "            pred[i_s:i_e] = results[0]\n",
    "            cost += (results[1] / (i_e-i_s))\n",
    "            time.sleep(0.01)\n",
    "            \n",
    "        return pred, cost\n",
    "    \n",
    "    def run_on_batch_wm(self, inputs, targets, mb_sz = 32):\n",
    "        data_count = inputs.shape[0]\n",
    "        pred = np.zeros((data_count, self.n_class))\n",
    "        cost = 0\n",
    "        \n",
    "        auc_variables = [ v for v in tf.local_variables() if v.name.startswith( 'auc' ) ]\n",
    "        sess.run(tf.initialize_variables(auc_variables))\n",
    "        f1_variables = [ v for v in tf.local_variables() if v.name.startswith( 'f1' ) ]\n",
    "        sess.run(tf.initialize_variables(f1_variables))\n",
    "        \n",
    "        for i in range(0, data_count, mb_sz):\n",
    "            i_s = i\n",
    "            i_e = np.minimum(data_count, i + mb_sz)\n",
    "            \n",
    "            sess_out = [self.pred, self.cost]\n",
    "            sess_out.extend(self.auc_op_list)\n",
    "            sess_out.extend(self.f1s_op_list)\n",
    "            feed_dict={self.inputs: inputs[i_s:i_e], self.targets: targets[i_s:i_e]}\n",
    "            results = sess.run(sess_out, feed_dict=feed_dict)\n",
    "            pred[i_s:i_e] = results[0]\n",
    "            cost += (results[1] / (i_e-i_s))\n",
    "            time.sleep(0.01)\n",
    "            \n",
    "        sess_out = [self.avg_auc, self.avg_f1s]\n",
    "        results = sess.run(sess_out, feed_dict={})\n",
    "        auc = results[0]\n",
    "        f1s = results[1]\n",
    "        \n",
    "        return pred, cost, auc, f1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable parameters: 270270\n",
      "WARNING:tensorflow:From C:\\Users\\ajit\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:118: initialize_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.variables_initializer` instead.\n",
      "Epoch: 1/20... trn loss: 73.20692 tst loss: 15.07689\n",
      "            ... trn auc:  0.87608 tst auc:  0.86892\n",
      "            ... trn f1s:  0.51047 tst f1s:  0.49919\n",
      "Epoch: 2/20... trn loss: 60.07050 tst loss: 12.99995\n",
      "            ... trn auc:  0.91213 tst auc:  0.89777\n",
      "            ... trn f1s:  0.58689 tst f1s:  0.55841\n",
      "Epoch: 3/20... trn loss: 54.84756 tst loss: 12.42193\n",
      "            ... trn auc:  0.92673 tst auc:  0.90719\n",
      "            ... trn f1s:  0.62568 tst f1s:  0.58767\n",
      "Epoch: 4/20... trn loss: 51.95334 tst loss: 12.32841\n",
      "            ... trn auc:  0.93417 tst auc:  0.90999\n",
      "            ... trn f1s:  0.64581 tst f1s:  0.59429\n",
      "Epoch: 5/20... trn loss: 50.38515 tst loss: 12.33060\n",
      "            ... trn auc:  0.93942 tst auc:  0.91274\n",
      "            ... trn f1s:  0.66300 tst f1s:  0.60213\n",
      "Epoch: 6/20... trn loss: 48.91837 tst loss: 12.40751\n",
      "            ... trn auc:  0.94206 tst auc:  0.91200\n",
      "            ... trn f1s:  0.66999 tst f1s:  0.60706\n",
      "Epoch: 7/20... trn loss: 44.01318 tst loss: 12.32240\n",
      "            ... trn auc:  0.95187 tst auc:  0.91541\n",
      "            ... trn f1s:  0.70322 tst f1s:  0.61171\n"
     ]
    }
   ],
   "source": [
    "### clear the graph\n",
    "tf.reset_default_graph()\n",
    "### build the network\n",
    "net = conv_net(im_size = 32, n_chan = 3, n_class = 10, learning_rate = 1e-3)\n",
    "net.parameter_count()\n",
    "### initialze the session and variables\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer()) \n",
    "\n",
    "epochs = 20\n",
    "for e in range(epochs):\n",
    "    nu.shuffle_together(x_train, onehot_train)\n",
    "    net.train_on_batch(x_train, onehot_train)\n",
    "#     _, trn_cost = net.run_on_batch(x_train, onehot_train)\n",
    "#     _, tst_cost = net.run_on_batch(x_test, onehot_test)\n",
    "    _, trn_cost, trn_auc, trn_f1s = net.run_on_batch_wm(x_train, onehot_train)\n",
    "    _, tst_cost, tst_auc, tst_f1s = net.run_on_batch_wm(x_test, onehot_test)\n",
    "    \n",
    "    print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "          \"trn loss: {:.5f}\".format(trn_cost),\n",
    "          \"tst loss: {:.5f}\".format(tst_cost))\n",
    "    print(\"            ...\".format(e+1, epochs),\n",
    "          \"trn auc:  {:.5f}\".format(trn_auc),\n",
    "          \"tst auc:  {:.5f}\".format(tst_auc))\n",
    "    print(\"            ...\".format(e+1, epochs),\n",
    "          \"trn f1s:  {:.5f}\".format(trn_f1s),\n",
    "          \"tst f1s:  {:.5f}\".format(tst_f1s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
